%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{listings}

\graphicspath{{fig/}}


\newcommand{\todo}[1]{\textcolor{red}{\textbf{#1}}}

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2024}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Parameter-Efficient Fine-Tuning of Large Language Models} 

% Authors (student competitors) and their info
\Authors{Ondřej Komín, Andrej Sušnik, Eileen Vu}

% Advisors
\affiliation{\textit{Advisors: Boshko Koloski, Slavko Žitnik}}

% Keywords
\Keywords{PEFT, LoRA, Prompt tuning, IA3, BitFit, CommonsenseQA, SloSuperGlue, CoNLL-2012, SST5}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
In this paper we present different parameter efficient fine tuning methods, i.e. LoRA, Soft prompts tuning, IA3 and Bitfit, and apply them to various NLP tasks.  Our evaluation reveals that Soft Prompts tuning, in certain tasks, can surpass full fine-tuning while requiring less training time. Additionally, we observe that LoRA finds a good balance between performance and training time across different tasks.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------
\section*{Introduction}
\input{content/1_intro}
%------------------------------------------------
\section*{Related Work}
\input{content/2_relatedwork}
%------------------------------------------------
\section*{Methods}
\input{content/3_0_methods}
%------------------------------------------------
% \section*{Model}
% Recommendation by Tutor: multilingual DeBERTA v3, sloBERTA, BERTIC (all Balkan languages), CROSLOBERTA
% Baseline: normal fine-tuning
% q Lora (half of the bits than LORA)

% other benchmarks: XGLUE (but our benchmarks should be fine)

% adapters
% - have one NN
% - have multiple adapters for each task
% - is implemented already

% next steps:
% - take slvoenian dataset
% - choose model
% - implement fine-tuning
% - use libraries if available

% until next time:
% - have three methods and finetuning impelemnted
% - for slovenian dataset
% %------------------------------------------------
\section*{Training Pipeline}
\input{content/4_training_pipeline}
%------------------------------------------------
\section*{Environment and reproducibility}
\input{content/4_2_envionment_and_reproducibility}
%------------------------------------------------
\section*{Results}
\input{content/5_0_results}
%------------------------------------------------
\section*{Discussion}
\input{content/6_discussion}
%------------------------------------------------
\section*{Conclusion}
\input{content/7_future_directions}
%------------------------------------------------

% \section*{Backup}
% \input{content/backup}
%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}

\end{document}