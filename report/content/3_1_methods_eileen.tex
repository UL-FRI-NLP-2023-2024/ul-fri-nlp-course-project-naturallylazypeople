\cite{xu2023parameterefficient} \textcolor{red}{this paper presents existing methods and compares them in experiments. Basically, exactly what we want to do as well. Just more comprehensive}
 General overview:
\begin{itemize}
    \item  comprehensive and systematic review of
    PEFT (parameter-efficient fine-tuning method) methods for PLMs (pre-trained language models)
    \item overall objective: reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning    
    \item full fine-tuning: e model is trained from scratch for the specific target task (expensive, prone to overfitting)
    \item PEFT: selectively updating or modifying specific parts of the PLMs 
\end{itemize}
Five categories of PEFTs:
\begin{enumerate}
    \item \textbf{additive fine-tuning}: introducing new extra trainable parameters for task-specific fine-tuning
    \begin{enumerate}
        \item \textbf{Adapter-based Fine-tuning}: adapter module is incorporated into the transformer, allowing for fine-tuning without modifying the pretrained parameters 
        % (\textit{e.g. Sequential Adapter, Residual Adapter, Parallel Adapter, AdapterDrop, CoDA, Tiny-Attn Adapter, AdapterFusion , MerA, Hyperformer++, AdapterSoup})
        \item \textbf{\textcolor{red}{Soft Prompt-based Fine-tuning}}: trainable continuous vectors, known as soft prompts, are inserted into the input or hidden state of the model. Unlike manually designed hard prompts, soft prompts are generated by searching for prompts in a discrete token space based on task-specific training data 
        % \textit{(e.g. WARP, Prompt-Tuning, Prefix-tuning, P-tuning, SPOT, ATTEMPT, MPT)}
        \item \textbf{Others} 
    \end{enumerate}
    \item \textbf{Partial Fine-tuning}: aims at reducing the number of fine-tuned parameters by selecting a subset of pre-trained parameters that are critical to downstream tasks while discarding unimportant ones
    \begin{enumerate}
        \item \textbf{Bias Update}: only the bias term in the attention layer, feed-forward layer and layer normalization of the transformer is updated
        \item \textbf{Pretrained Weight Masking}: where the pretrained weights are masked using various pruning criterion
        \item \textbf{Delta Weight Masking}: delta weights are masked via pruning techniques and optimization approximation
    \end{enumerate}
    \item \textbf{Reparameterized Fine-tuning}: utilizing low-rank transformation to reduce the number of trainable parameters while allowing operating with high-dimensional matrices (e.g., pretrained weights)
    \begin{enumerate}
        \item \textbf{Low-rank Decomposition}: s finding a lowerrank matrix that captures the essential information of the original matrix while reducing computational complexity and memory usage by reparameterizing the updated delta weight
        \item \textbf{\textcolor{red}{LoRA Derivatives}}: series of PEFT methods that are improved based on LoRA
    \end{enumerate}
    \item \textbf{Hybrid Fine-Tuning}: aim to combine various PEFT approaches, such as adapter, prefix-tuning, and LoRA, to leverage the strengths of each method and mitigate their weaknesses
    \begin{enumerate}
        \item \textbf{Manual Combination}: mainly involves integrating the structure or features of one PEFT method into another PEFT method to enhance performance while achieving parameter efficiency
        \item \textbf{\textcolor{red}{Automatic Combination}}: explores how to configure PEFT methods like adapters, prefixtuning, BitFit, and LoRA to different layers of the transformers automatically using various structure search and optimization approaches
    \end{enumerate}
    \item \textbf{Unified Fine-tuning}: unified framework for finetuning, which streamlines the incorporation of diverse finetuning methods into a cohesive architecture, ensuring consistency and efficiency across the adaptation and optimization of models. Unlike hybrid fine-tuning methods, unified fine-tuning methods typically utilize a single PEFT method rather than a combination of various PEFT methods.
\end{enumerate}

\noindent\todo{EIleen} \cite{zaken2022bitfit} BitFit Overview:
\begin{itemize}
    \item only the bias-terms of the model (or a subset of them) are being modified
    \item  freezing most of the network and fine-tuning only the bias-terms is surprisingly effective
    \item Three key properties
    \begin{enumerate}
        \item match the results of fully fine-tuned model
        \item enable tasks to arrive in a stream, this way it does not require simultaneous access to all datasets
        \item  fine-tune only a small portion of the modelâ€™s parameters
    \end{enumerate}
    \item trains less than 0.1\% of the total number of parameters
    \item Bitfit achieves transfer learning performance which is comparable (and sometimes better!) than fine-tuning of the entire network
\end{itemize}