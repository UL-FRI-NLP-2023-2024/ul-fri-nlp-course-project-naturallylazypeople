Fine-tuning LLMs plays a crucial role in adapting these models to domain-specific tasks. As LLMs are pre-trained on vast amounts of text data, they capture linguistic patterns and semantic information. However, for tasks with specific requirements, such as sentiment analysis or named entity recognition, fine-tuning allows these models to tailor their representations to better suit the task at hand. Traditional methods of full fine-tuning involve updating all parameters of the pre-trained LLM. The popular GPT-4 model released in early 2024 contains 1.76 trillion weights \cite{openai2024gpt4}. Fine-tuning (and storing) this amount of parameters whenever one wants to apply the model to a specific use case however not only requires significant computational resources but also poses a risk of overfitting, especially in scenarios with limited task-specific data.

\noindent In order to avoid these problems, PEFT methods have emerged as a solution to the drawbacks of full fine-tuning. By reducing the number of parameters updated during fine-tuning, PEFT not only mitigates computational costs but also helps alleviate overfitting concerns. As described in \cite{xu2023parameterefficient}, PEFT techniques can be divided into five main categories: additive fine-tuning, partial fine-tuning, reparameterized fine-tuning, hybrid fine-tuning and lastly unified fine-tuning. While some of these methods aim at introducing new trainable parameters for use-case-specific fine-tuning, others reduce the number of trainable parameters by transforming the weights into lower dimensions.

In this paper, we focus on presenting and comparing four PEFT methods in the context of different NLP tasks. Specifically, we investigate the performance of the following methodologies: low rank adaptation (LoRA), soft prompt-based fine-tuning, reparameterized fine-tuning and partial fine-tuning. \textbf{LoRA} \cite{hu2021lora} has become very popular in the last years due to its ability to reduce the number of parameters without introducing additional latency, unlike for example adapter methods. This lowers training computational requirements while improving performance for specific NLP tasks. \textbf{Soft-prompting} \cite{lester2021power} is a machine learning technique that offers subtle guidance to models during training, aiding in learning without the need for explicit labels. This approach is valuable as it allows for more flexible decision-making while still achieving desired outcomes, especially in scenarios where labeled data may be scarce or costly to obtain. \textbf{IA3} \cite{liu2022fewshot} is a reparameterized fine-tuning technique for pretrained language models. It rescales inner activations with learned vectors, reducing the number of trainable parameters while optimizing model performance for task-specific data. Lastly, we will investigate the partial fine-tuning methods \textbf{BitFit} \cite{zaken2022bitfit}. This method only fine-tunes the bias term of the layers while freezing the rest of the network. This technique, which trains less than 0.1\% of the total number of weights, was proven to achieve comparable performance than full fine-tuning. 

\begin{table}[htbp]
  \centering
  \begin{tabularx}{\columnwidth}{lX}
    \toprule
    \textbf{Benchmark} & \textbf{NLP Task} \\
    \midrule
    CommonsenseQA \cite{talmor2019commonsenseqa} & Commonsense Reasoning \\
    CoNLL-2012 \cite{pradhan2012conll} & Coreference Resolution \\
    % XSum \cite{narayan2018xsum} & Text Summarization \\
    SST5 \cite{maas2011sentiment} & Sentiment Analysis \\
    Slovene SuperGLUE \cite{robnik2022superglue} & Slovene BoolQ (Boolean Questions) \\
    XSum \cite{narayan2018xsum} & Text Summarization \\
    \bottomrule
  \end{tabularx}
  \caption{Chosen benchmarks for performance evaluation}
  \label{tab:benchmarks}
\end{table}
We will provide an empirical comparison of these three methodologies based on four different NLP tasks\footnote{XSum not implemented}. The benchmarks we have chosen for this each represent distinct natural language understanding skills allowing us to provide a comprehensive overview of the advantages and disadvantages of all techniques.