\paragraph{LoRA}

LoRA \cite{hu2021lora} is a method designed for fine-tuning large models. It operates by fixing the weights of the original model and introducing a trainable low-rank decomposition matrix into the LLM architecture. This modified architecture involves fixing the original weights $W_0$ while introducing additional trainable weights $\Delta W$, which can be decomposed into matrices $BA$, where the rank of both $B$ and $A$ is much smaller than that of $W_0$. Consequently, the resulting weights are formulated as $W0 + BA$, significantly reducing the number of parameters that need to be trained. 

Empirical results indicate that LoRA performs comparably or even better than other methods, while requiring a comparable or lower number of trainable parameters. Notably, LoRA drastically reduces the number of trainable parameters, such as in the case of fine-tuning the GPT-3 model, where the parameter count was reduced by 10000 times, accompanied by a threefold reduction in GPU memory requirement. Additionally, LoRA exhibits several benefits, including the ability to train specialized models without introducing latency, as seen in adapter methods. 

Moreover, it enables the deployment of multiple specialized models simultaneously by reducing memory and computational footprint, achieved through maintaining fixed weights for the base model while having several trained decomposition matrices for each specialized task.

LoRa model is trained with configuration showed in Listing \ref{lst:lora_params}.

\begin{lstlisting}[language=Python, caption={LoRa parameters}, label={lst:lora_params}]
lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.1,
        bias="lora_only",
        task_type="SEQ_CLS"
)
\end{lstlisting}