In conclusion, our investigation into PEFT methods across various NLP tasks reveals significant advantages in terms of resource efficiency without compromising task performance. Specifically, the LoRA method consistently demonstrates remarkable reductions in RAM usage and parameter count compared to full fine-tuning, while maintaining competitive or even superior performance across different models and languages. Soft Prompts also show promise, particularly in achieving high accuracy and recall, indicating effective identification of correct answers with minimal resource requirements. These findings underscore the potential of PEFT methods in enhancing model efficiency and scalability, paving the way for more sustainable and accessible NLP solutions.